## 2017.2.8

1. cross_entropy:什么意思？典型应用场景？

2. 激活函数必须是可微分的，因为在反向传播的过程中，需要使用微分来计算梯度从而更新参数。

3. dropout解决过拟合是为了让每个神经元的每条边上的权重尽量均衡，不会出现过大的权重。就像非神经网络形式的模型中的正则化一样，L1, L2……正则化分别加入对权重（一次，二次……）的惩罚项，避免单个权重过大，使得模型过分依赖某一参数？这个感觉又说不太通……

4. 各种optimizer都是在参数更新的过程中动了手脚，有时候对学习率动手脚，有时候对更新的微分动手脚，有时候两者融合。Adam就是两者融合的参数更新方式，实验总结又快又好！！

5. 在激活函数和输出之间添加batch_normalization是极好的，可以让神经元的活性增大！让大部分数据处于神经元比较活跃的区间，就是非饱满的区间，也就是可以产生较大的delta的区间！


---

## 2017.3.9：

1. 语言模型的进化史：
   N-gram：用频率代替概率，计算一下就可以得到每个句子、词组的概率（联合分布），然后就可以通过输入计算后验概率得到具有最大后验概率的句子就是输出。
   Log-linear:


---

## 2017.3.14：

1.  知识库补全：根据知识库已有的知识，提取新的知识，并添加到知识库中。
    
    关系提取：学习知识库没有的知识。

2. 远程监督：
    
    如果一个句子中出现两个实体，同时两个实体在知识库中存在关系，就把这个句子加上label为改关系，这个假设太强，易带来噪音。

3. scalar_answer和lookup_answer，训练的时候怎么区分的？？？

4. 为什么embedding被称为distributed representation？？分散表达式什么概念？？？


## 2017.5.27

---

* 前馈神经网的一层
$$
y = f \left ( \mathbf{W} \cdot \mathbf{x} \right) 
$$


* LSTM

    * 遗忘门

    $$
    \mathbf{f_t} = \sigma \left( \mathbf{W_f} \cdot \left[ \mathbf{H_{t-1}}, \mathbf{x_{t}} \right]  \right)
    $$

    * 输入门
    $$
    \mathbf{i_t} = \sigma \left( \mathbf{W_i} \cdot \left[ \mathbf{H_{t-1}}, \mathbf{x_{t}} \right]  \right)
    $$
    
    * 输出门：
    $$
    \mathbf{o_t} = \sigma \left( \mathbf{W_o} \cdot \left[ \mathbf{H_{t-1}}, \mathbf{x_{t}} \right]  \right)
    $$
    
    * 候选
    $$
    \tilde { \mathbf{C_t}} = tanh \left( \mathbf{W_c} \cdot \left[ \mathbf{H_{t-1}}, \mathbf{x_{t}} \right]  \right) \mathbf{C_t} =\mathbf {f_t} \ast \mathbf {C_{t-1}} + \mathbf{i_t} \ast \tilde \mathbf{C_t} 
    $$
    
    * 隐状态
    $$
    \mathbf{h_t} =\mathbf {o_t} \ast tanh \left ( \mathbf {C_t} \right)
    $$

