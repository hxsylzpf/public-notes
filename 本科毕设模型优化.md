## 2016.9.10

* n折交叉验证n-fold cross validation

    就是每次实验先将数据集分成n份，进行n次训练，每次使用其中1份作为测试集，其余作为训练集训练模型，每次训练都会得到一个测试指标，取n次测试指标的平均值作为本次实验的测试指标的最终值。由于我们设计的实验室随机实验，因此要多测几组，这样就会大大增加实验花费的时间！因此修改了模型的收敛条件（两次训练之后计算得到的loss的差值的绝对值小于0.5，则停止训练！）方法懂了，尚未实现，因为筛数据这一环节更重要，现在还在筛数据，间或地实现这部分。


---

## 2016.9.12

p@n和交叉验证都加入模型中了，本次运行使用的是原来的数据集（全部都是music话题）， 由于两次的loss差值的绝对值取得比较大，收敛判定太早，虽然实验代码运行加快很多，但是评价指标都不好。已将loss差值的绝对值从0.1改为0.01（仅限纯bpr版本，其他版本尚未运行结束）。

---

## 2016.9.13

1. 经查看，发现纯bpr收敛过快是因为迭代次数设置错误，已经更改并重新放到服务器上运行。p@n计算的不是均值，可能稍微有点问题，目前正在解决。


2. p@n与 map的区别和联系

    p@n的计算前n个预测值中属于真实值的比例。计算map多次用到p@n。比如其中AveP就是对前limit个推荐一次求p@n，然后就求均值。
    现在map的公式：

    $$
    AveP = \frac{\sum_{k = 1}^{limit} \left( p_k \times rel \left( k \right) \right)} {\left| S_{rel} \right|}
    $$
    其中
    $$
    rel\left( k \right) = \begin{cases} 0, & {u_k \notin S_{rel} } \\ 1, & {u_k \in S_{rel}} \end{cases}
    $$
    其中 `$S_{rel}$ ` 表示当前活动的所有正样本集合。`limit`是自己选的，这里设置为10，表示用户最终得到的推荐列表长度为10，就是对除训练集之外的正样本用户排序得到的前10个。

    $$
    p_k = \frac{len\left\{  {u\left| u_m \in S_{rel} \text {  and  }  1 \leq m \leq k \right.}  \right\}} {k}

    map = \frac {\sum_{i \in T_e} AveP_i} {\left| {T_e} \right|}
    $$
    其中`$T_e$`表示测试集中的活动集合，经和江乐师兄讨论，新版本的map计算方法是正确的。

3. 重新筛数据，准备方案如下：

    * 随机抽取一个活动，对所有活动按相似度（score）排序，取出前k（设定为1000）个，然后
      找出参加过这些活动之一的所有用户的集合，最后以用户-活动对的形式存储用户和活动的关系。

    * 筛选出参与人数在一定范围（暂定10~20）的活动，然后找出参加过这些活动之一所有用户的的集合，最后以用户-活动对的形式存储用户和活动的关系。经验证，这种筛选数据的方式并不靠谱，融入进来之后的测试指标还不如不加近邻的测试指标！

经过新一轮的筛选，得到的数据全是salon主题数据，auc达到了0.72，但是纯bpr和带近邻的bpt没有区别，还要继续筛数据。

4. 争取auc达到0.8，map达到0.1

5. 开始写一份代码之前，一定要把代码的作用写清楚，要做什么，目的是什么，主要过程是什么样，必须做好这一步！最好先在纸上写一下，在这个过程中会发现一些问题，而且会促使自己去优化思路。

6. 每次新加版本之前要将现有代码完整拷贝，新建工程！然后对工程进行深拷贝！

7. 新增计算p@n的接口，实现的功能是计算p@1~p@20，并将所有的值存入文件。

8. 9月10日晚运行期间，服务器磁盘空间溢出，程序运行中断，现已重新运行。


---

## 2016.9.16

1. 带近邻的BPR运行10次的AUC：
    ```
    0.6374191181252998
    0.6827881106791406
    0.6790607207126994
    0.67803077897787
    0.6742804685554145
    0.680813449363512
    0.6751340991748886
    0.672704269463673
    0.6829428705683371
    0.668982951151504
    0.6611114387474386
    ```
    第一次运行得到的AUC较低，推测代码仍然存在问题，需要继续跟进。


2. 近邻发现部分，重新做一个版本，使用集合方法
    经验证，auc和map两个指标都和使用score版本的算法相差较远，这可能是因为数据集的问题，本次实验所使用的数据集中所有活动都属于一个category，这并不合理！这些数据、代码和实验的结果都保存在服务器上的bpr/record1文件夹下，方便以后复现！

3. ==最新的map计算代码：==
    ```python

    average_precision_sum = 0

    test_data_dict = {}
    # TODO: change test set from list to dict
    for piece in self.test_data:
        if piece[0] in test_data_dict:
            test_data_dict[piece[0]].append(piece[1])
        else:
            test_data_dict[piece[0]] = []
            test_data_dict[piece[0]].append(piece[1])

    for key in test_data_dict:
        all_order = self.event_negative_users_dict[key]
        for u in test_data_dict[key]:
            all_order.append(u)

        user_influence_dict = {}
        for user in all_order:
            user_influence_dict[user] = np.dot(self.event_factors[key], self.user_factors[user]) \
                                        + self.user_bias[user]
        sorted_users_by_influence = sorted(user_influence_dict.items(), key=lambda d: d[1], reverse=True)
        sorted_user_list = []
        for i in sorted_users_by_influence:
            sorted_user_list.append(i[0])

        predict_n = 0
        for i in range(10):
            counter = 0
            for j in range(i + 1):
                if sorted_user_list[j] in test_data_dict[key]:
                    counter += 1.0
            if sorted_user_list[i] in test_data_dict[key]:
                predict_n += counter / (i + 1)

        average_precision_sum += predict_n / len(test_data_dict[key])

    map_mark = average_precision_sum / len(test_data_dict)
    return map_mark

    ```

    ==原版本map计算代码：==
    ```python
        average_precision_sum = 0
    real_order = get_real_social_influence_order()

    for piece in self.test_data:
        event_id = piece[0]
        user_list = range(self.num_users)

        user_influence_dict = {}
        for user in user_list:
            user_influence_dict[user] = np.dot(self.event_factors[event_id], self.user_factors[user]) \
                                        + self.user_bias[user]
        sorted_users_by_influence = sorted(user_influence_dict.items(), key=lambda d: d[1], reverse=True)
        sorted_user_list = []
        for i in sorted_users_by_influence:
            sorted_user_list.append(i[0])

        # calculate p@n

        limit = 10
        if len(real_order[event_id]) < 10:
            limit = len(real_order[event_id])
        real_order_list = []
        if len(real_order[event_id]) < 10:
            real_order_list = real_order
        else:
            for i in range(10):
                real_order_list.append(real_order[i])

        predict_n = 0
        valid_n = 0
        for i in range(10):
            counter = -1
            for j in range(10):
                if real_order[event_id][i] == sorted_user_list[j]:
                    counter = j
                    break
            if counter != -1:
                valid_n += 1.0
                predict_n += 1.0 / (1.0 + counter)
        if valid_n != 0:
            predict_n /= valid_n
        event_sum += predict_n
    map_mark = event_sum / len(self.test_data)

    # the lines above to calculate is wrong. it should be like the following lines
        if len(real_order[event_id]) > 10:
            real_order = real_order[0:10]

        predict_n = 0
        for i in range(10):
            counter = 0
            for j in range(i + 1):
                if sorted_user_list[j] in real_order:
                    counter += 1.0
            if sorted_user_list[i] in real_order:
                predict_n += counter/(i + 1)

        in_counter = 0
        for i in range(10):
            if sorted_user_list[i] in real_order:
                in_counter += 1.0

        average_precision_sum += predict_n / in_counter
    map_mark = average_precision_sum / len(test_data)
    return map_mark
    ```
